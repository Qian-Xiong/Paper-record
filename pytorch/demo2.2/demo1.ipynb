{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor([1.2, 3.4]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "torch.tensor([1.2, 3.4]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.dtype: torch.float64\n",
      "a.long()方法： torch.int64\n",
      "a.int()方法： torch.int32\n",
      "a.float()方法： torch.float32\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.2, 3.4])\n",
    "print(\"a.dtype:\", a.dtype)\n",
    "print(\"a.long()方法：\", a.long().dtype)\n",
    "print(\"a.int()方法：\", a.int().dtype)\n",
    "print(\"a.float()方法：\", a.float().dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_tensor_type(torch.FloatTensor)  #恢复数据类型\n",
    "torch.tensor([1.2, 3.4]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_default_dtype()  #获取默认数据类型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.2000, 1.3000],\n        [3.4000, 3.5000]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#生成tensor\n",
    "a = torch.tensor([[1.2, 1.3], [3.4, 3.5]])\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 2])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 2])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.numel()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 2., 3., 4.], requires_grad=True)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor((1, 2, 3, 4), dtype=torch.float32, requires_grad=True)  #设置张量有梯度，只有浮点型才能计算梯度\n",
    "b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2., 4., 6., 8.])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = b.pow(2).sum()\n",
    "y.backward()\n",
    "b.grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 2., 3., 4.])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用torch.Tensor()函数创建张量\n",
    "c = torch.Tensor([1, 2, 3, 4])\n",
    "c"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1.3237e+22, 6.6820e+22, 2.0682e+20],\n         [2.6368e-09, 4.1915e+21, 1.6778e+22]]),)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.Tensor(2, 3)\n",
    "d,"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) tensor([[0.1795, 0.9908, 0.0848],\n",
      "        [0.6829, 0.6210, 0.4237]])\n"
     ]
    }
   ],
   "source": [
    "#torch.ones_like() torch.zero() torch.rand()函数\n",
    "e = torch.ones_like(d)\n",
    "f = torch.zeros_like(d)\n",
    "g = torch.rand_like(d)\n",
    "print(e, f, g)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17356\\1493618824.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  h=d.new_tensor(d)\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[1.3237e+22, 6.6820e+22, 2.0682e+20],\n        [2.6368e-09, 4.1915e+21, 1.6778e+22]])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d.new_**()类函数\n",
    "h = d.new_tensor(d)\n",
    "h"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]], dtype=torch.float64)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#张量与numpy转化\n",
    "import numpy as np\n",
    "\n",
    "i = np.ones((3, 4))\n",
    "iTensor = torch.as_tensor(i)\n",
    "iTensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]], dtype=torch.float64)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iTensor = torch.from_numpy(i)\n",
    "iTensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-0.1115)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "a = torch.normal(mean=0.0, std=torch.tensor(1.0))\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.1115,  0.2407, -1.1089, -0.9617])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "a = torch.normal(mean=0.0, std=torch.arange(1, 5.0))\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.2961, 0.5166, 0.2517, 0.6886],\n        [0.0740, 0.8665, 0.1366, 0.1025],\n        [0.1841, 0.7264, 0.3153, 0.6871]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "a = torch.rand(3, 4)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.2961, 0.5166, 0.2517],\n",
      "        [0.6886, 0.0740, 0.8665]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "c = torch.ones(2, 3)\n",
    "print(c)\n",
    "d = torch.rand_like(c)\n",
    "print(d)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9447,  0.6217, -1.3501],\n",
      "        [-0.1881, -2.3891, -0.4759],\n",
      "        [ 1.7603,  0.6547,  0.5490]])\n",
      "tensor([[ 0.3671,  0.1219,  0.6466],\n",
      "        [-1.4168,  0.8429, -0.6307]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randn(3, 3))\n",
    "print(torch.randn_like(c))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2, 0, 8, 1, 3, 7, 4, 9, 5, 6])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.randperm(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 2, 4, 6, 8])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##使用torch.arange()生成张量\n",
    "torch.arange(start=0, end=10, step=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 1.0000,  3.2500,  5.5000,  7.7500, 10.0000])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(start=1, end=10, steps=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 1.2589,  2.1135,  3.5481,  5.9566, 10.0000])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logspace(start=0.1, end=1.0, steps=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##使用tensor.reshape()方法设置张量的形状大小\n",
    "a = torch.arange(12.0).reshape(3, 4)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n        [ 6.,  7.,  8.,  9., 10., 11.]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(input=a, shape=(2, -1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n        [ 6.,  7.,  8.,  9., 10., 11.]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##使用resize_()方法\n",
    "a.resize_(2, 6)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 1., 2.],\n        [3., 4., 5.],\n        [6., 7., 8.]])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.arange(10.0, 19.0).reshape(3, 3)\n",
    "a.resize_as_(b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[10., 11., 12.],\n        [13., 14., 15.],\n        [16., 17., 18.]])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([1, 2, 6])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##torch.unsqueeze()函数在指定维度插入尺寸为1的新张量\n",
    "a = torch.arange(12.0).reshape(2, 6)\n",
    "print(a.shape)\n",
    "b = torch.unsqueeze(a, dim=0)\n",
    "b.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c.shape: torch.Size([1, 2, 6, 1])\n",
      "d.shape: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "##torch.squeeze()移除所有维度为1的维度\n",
    "c = b.unsqueeze(dim=3)\n",
    "print(\"c.shape:\", c.shape)\n",
    "d = torch.squeeze(c)\n",
    "print(\"d.shape:\", d.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e.shape torch.Size([1, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "##起初指定维度为1的维度\n",
    "e = torch.squeeze(c, dim=3)\n",
    "print(\"e.shape\", e.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.,  0.,  1.,  2.,  3.,  4.,  5.],\n",
      "         [ 6.,  7.,  8.,  9., 10., 11.,  6.,  7.,  8.,  9., 10., 11.],\n",
      "         [ 0.,  1.,  2.,  3.,  4.,  5.,  0.,  1.,  2.,  3.,  4.,  5.],\n",
      "         [ 6.,  7.,  8.,  9., 10., 11.,  6.,  7.,  8.,  9., 10., 11.]]])\n",
      "torch.Size([1, 4, 12])\n"
     ]
    }
   ],
   "source": [
    "##使用.repeat()方法拓展张量\n",
    "d = b.repeat(1, 2, 2)\n",
    "print(d)\n",
    "print(d.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11]]])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##利用切片和索引获取张量中的元素\n",
    "a = torch.arange(12).reshape(1, 3, 4)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1, 2, 3],\n        [4, 5, 6, 7]])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##获取第0维度下的矩阵的前两行元素\n",
    "a[0, 0:2, :]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 8,  9, 10])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##获取第0维度下的矩阵，最后一行-4,-1lie\n",
    "a[0, -1, -4:-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0, -1, -2, -3],\n         [-4, -5,  6,  7],\n         [ 8,  9, 10, 11]]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##根据条件筛选\n",
    "b = -a\n",
    "torch.where(a > 5, a, b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 6,  7,  8,  9, 10, 11])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##获取a中大于5的元素\n",
    "a[a > 5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  0,  0,  0],\n         [ 4,  5,  0,  0],\n         [ 8,  9, 10,  0]]])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##获取矩阵张量的下三角部分\n",
    "torch.tril(a, diagonal=0, )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  1,  2,  3],\n         [ 0,  0,  6,  7],\n         [ 0,  0,  0, 11]]])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(a, diagonal=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  1,  0,  0],\n         [ 4,  5,  6,  0],\n         [ 8,  9, 10, 11]]])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##diagonal参数控制要考虑的对角线\n",
    "torch.tril(a, diagonal=1, )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([ 0,  5, 10])\n",
      "tensor([ 1,  6, 11])\n"
     ]
    }
   ],
   "source": [
    "##获取矩阵张量的对角线元素，input需要是一个二维张量\n",
    "c = a.reshape(3, 4)\n",
    "print(c)\n",
    "print(torch.diag(c, diagonal=0))\n",
    "print(torch.diag(c, diagonal=1))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 0, 0],\n        [0, 2, 0],\n        [0, 0, 3]])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##提供对角线元素生成矩阵张量\n",
    "torch.diag(torch.tensor([1, 2, 3]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  2.],\n        [ 3.,  4.,  5.],\n        [ 0.,  2.,  4.],\n        [ 6.,  8., 10.]])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6.0).reshape(2, 3)\n",
    "b = torch.linspace(0, 10, 6).reshape(2, 3)\n",
    "c = torch.cat((a, b), dim=0)\n",
    "c"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  2.,  0.,  2.,  4.],\n        [ 3.,  4.,  5.,  6.,  8., 10.]])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##在1维度连接张量\n",
    "d = torch.cat((a, b), dim=1)\n",
    "d"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  0.,  1.,  2.,  0.,  2.,  4.],\n        [ 3.,  4.,  3.,  4.,  5.,  6.,  8., 10.]])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = torch.cat((a[:, 0:2], a, b), dim=1)\n",
    "e"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.,  0.,  1.,  2.,  0.,  2.,  4.],\n        [ 4.,  3.,  4.,  5.,  6.,  8., 10.]])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##在1维度链接3个张量\n",
    "e = torch.cat((a[:, 1:2], a, b), dim=1)\n",
    "e"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 0.,  2.,  4.],\n",
      "         [ 6.,  8., 10.]]])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "f = torch.stack((a, b), dim=0)\n",
    "print(f)\n",
    "print(f.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  0.],\n",
      "         [ 1.,  2.],\n",
      "         [ 2.,  4.]],\n",
      "\n",
      "        [[ 3.,  6.],\n",
      "         [ 4.,  8.],\n",
      "         [ 5., 10.]]])\n",
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "g = torch.stack((a, b), dim=2)\n",
    "print(g)\n",
    "print(g.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1., 0., 1., 2., 0., 2., 4.]]),\n tensor([[ 4.,  3.,  4.,  5.,  6.,  8., 10.]]))"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##在行上将张量e分为两块\n",
    "torch.chunk(e, 2, dim=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[ 0.,  2.,  4.],\n",
      "        [ 6.,  8., 10.]])\n"
     ]
    }
   ],
   "source": [
    "d1, d2 = torch.chunk(d, 2, dim=1)\n",
    "print(d1)\n",
    "print(d2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  0.,  1.,  2.,  0.,  2.,  4.],\n",
      "        [ 4.,  3.,  4.,  5.,  6.,  8., 10.]])\n",
      "tensor([[1., 0., 1.],\n",
      "        [4., 3., 4.]])\n",
      "tensor([[2., 0., 2.],\n",
      "        [5., 6., 8.]])\n",
      "tensor([[ 4.],\n",
      "        [10.]])\n"
     ]
    }
   ],
   "source": [
    "##如果沿给定维度dim的张量大小不\n",
    "e1, e2, e3 = torch.chunk(e, 3, dim=1)\n",
    "print(e)\n",
    "print(e1)\n",
    "print(e2)\n",
    "print(e3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  0.,  2.,  4.],\n",
      "        [ 3.,  4.,  5.,  6.,  8., 10.]])\n",
      "tensor([[0.],\n",
      "        [3.]])\n",
      "tensor([[1., 2.],\n",
      "        [4., 5.]])\n",
      "tensor([[ 0.,  2.,  4.],\n",
      "        [ 6.,  8., 10.]])\n"
     ]
    }
   ],
   "source": [
    "d1, d2, d3 = torch.split(d, [1, 2, 3], dim=1)\n",
    "print(d)\n",
    "print(d1)\n",
    "print(d2)\n",
    "print(d3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "##比较两个数是否相近\n",
    "a = torch.tensor([10.0])\n",
    "b = torch.tensor([10.1])\n",
    "print(torch.allclose(a, b, rtol=5.0, atol=8.0, equal_nan=False))\n",
    "print(torch.allclose(a, b, rtol=0.1, atol=0.01, equal_nan=False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "##如果equal_nan=True,那么缺失值可以判断接近\n",
    "a = torch.tensor(float(\"nan\"))\n",
    "print(torch.allclose(a, a, equal_nan=False))\n",
    "print(torch.allclose(a, a, equal_nan=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True]])\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "##计算元素是否相等\n",
    "a = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "b = torch.arange(1, 7)\n",
    "c = torch.unsqueeze(b, dim=0)\n",
    "print(torch.eq(a, b))\n",
    "print(torch.eq(a, c))\n",
    "##判断两个张量是否具有相同的形状和元素\n",
    "print(torch.equal(a, b))\n",
    "print(torch.equal(a, c))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True]])\n",
      "tensor([False, False, False, False, False, False])\n",
      "tensor([[False, False, False, False, False, False]])\n",
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True]])\n",
      "tensor([False, False, False, False, False, False])\n",
      "tensor([[False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "##逐元素比较大于等于\n",
    "print(torch.ge(a, b))\n",
    "print(torch.ge(a, c))\n",
    "##逐元素比较大于\n",
    "print(torch.gt(a, b))\n",
    "print(torch.gt(a, c))\n",
    "##逐元素比较小于等于\n",
    "print(torch.le(a, b))\n",
    "print(torch.le(a, c))\n",
    "##逐元素比较不等于\n",
    "print(torch.ne(a, b))\n",
    "print(torch.ne(a, c))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "b: tensor([[10., 12., 14.],\n",
      "        [16., 18., 20.]])\n",
      "tensor([[  0.,  12.,  28.],\n",
      "        [ 48.,  72., 100.]])\n",
      "tensor([[0.0000, 0.0833, 0.1429],\n",
      "        [0.1875, 0.2222, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "##矩阵逐元素相乘\n",
    "a = torch.arange(6.0).reshape(2, 3)\n",
    "b = torch.linspace(10, 20, steps=6).reshape(2, 3)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(a * b)\n",
    "##逐元素相除\n",
    "print(a / b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 13., 16.],\n",
      "        [19., 22., 25.]])\n",
      "tensor([[-10., -11., -12.],\n",
      "        [-13., -14., -15.]])\n",
      "tensor([[inf, 12.,  7.],\n",
      "        [ 5.,  4.,  4.]])\n"
     ]
    }
   ],
   "source": [
    "##逐元素相加\n",
    "print(a + b)\n",
    "##逐元素相减\n",
    "print(a - b)\n",
    "##逐元素整除\n",
    "print(b // a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   1.,   8.],\n",
      "        [ 27.,  64., 125.]])\n",
      "tensor([[  0.,   1.,   8.],\n",
      "        [ 27.,  64., 125.]])\n"
     ]
    }
   ],
   "source": [
    "##张量的幂\n",
    "print(torch.pow(a, 3))\n",
    "print(a ** 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.0000,   2.7183,   7.3891],\n",
      "        [ 20.0855,  54.5981, 148.4132]])\n",
      "tensor([[  -inf, 0.0000, 0.6931],\n",
      "        [1.0986, 1.3863, 1.6094]])\n",
      "tensor([[0.0000, 1.0000, 1.4142],\n",
      "        [1.7321, 2.0000, 2.2361]])\n",
      "tensor([[0.0000, 1.0000, 1.4142],\n",
      "        [1.7321, 2.0000, 2.2361]])\n"
     ]
    }
   ],
   "source": [
    "##计算张量的指数\n",
    "print(torch.exp(a))\n",
    "##计算张量的对数\n",
    "print(torch.log(a))\n",
    "##计算张量的平方根\n",
    "print(torch.sqrt(a))\n",
    "print(a ** 0.5)\n",
    "##计算张量平方根倒数\n",
    "print(torch.rsqrt(a))\n",
    "print(1 / (a ** 0.5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 4.]])\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[2.5000, 2.5000, 2.5000],\n",
      "        [3.0000, 4.0000, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "##根据最大值裁剪\n",
    "print(torch.clamp_max(a, 4))\n",
    "##根据最小值裁剪\n",
    "print(torch.clamp_min(a, 3))\n",
    "##根据范围裁剪\n",
    "print(torch.clamp(a, 2.5, 4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 3.],\n",
      "        [1., 4.],\n",
      "        [2., 5.]])\n",
      "tensor([[ 5., 14.],\n",
      "        [14., 50.]])\n",
      "tensor([[[ 10.,  13.],\n",
      "         [ 28.,  40.]],\n",
      "\n",
      "        [[172., 193.],\n",
      "         [244., 274.]]])\n"
     ]
    }
   ],
   "source": [
    "##矩阵的转置\n",
    "c=torch.t(a)\n",
    "print(c)\n",
    "##矩阵运算,矩阵相乘,a的行数要等于c的列数\n",
    "print(a.matmul(c))\n",
    "a=torch.arange(12.0).reshape(2,2,3)\n",
    "b=torch.arange(12.0).reshape(2,3,2)\n",
    "ab=torch.matmul(a,b)\n",
    "print(ab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n"
     ]
    }
   ],
   "source": [
    "##矩阵相乘只计算最后面两个维度的乘法\n",
    "print(ab[0].eq(torch.matmul(a[0],b[0])))\n",
    "print(ab[1].eq(torch.matmul(a[1],b[1])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.0000e+00, 0.0000e+00, 2.3842e-07],\n        [0.0000e+00, 1.0000e+00, 2.3842e-07],\n        [0.0000e+00, 2.3842e-07, 1.0000e+00]])"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##计算矩阵的逆\n",
    "c=torch.rand(3,3)\n",
    "d=torch.inverse(c)\n",
    "torch.mm(c,d)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(12.)"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##计算张量矩阵的迹，对角线元素的和\n",
    "torch.trace(torch.arange(9.0).reshape(3,3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大值： tensor(99.)\n",
      "最大值位置： tensor(8)\n",
      "最小值: tensor(11.)\n",
      "最小值位置： tensor(3)\n"
     ]
    }
   ],
   "source": [
    "##一维张量的最大值和最小值\n",
    "a=torch.tensor([12.,34,25,11,67,32,29,30,99,55,23,44])\n",
    "##最大值及位置\n",
    "print(\"最大值：\",a.max())\n",
    "print(\"最大值位置：\",a.argmax())\n",
    "##最小值及位置\n",
    "print(\"最小值:\",a.min())\n",
    "print(\"最小值位置：\",a.argmin())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-d张量b:\n",
      " tensor([[12., 34., 25., 11.],\n",
      "        [67., 32., 29., 30.],\n",
      "        [99., 55., 23., 44.]])\n",
      "最大值:\n",
      " torch.return_types.max(\n",
      "values=tensor([34., 67., 99.]),\n",
      "indices=tensor([1, 0, 0]))\n",
      "最大值位置： tensor([1, 0, 0])\n",
      "最小值:\n",
      " torch.return_types.min(\n",
      "values=tensor([12., 32., 23., 11.]),\n",
      "indices=tensor([0, 1, 2, 0]))\n",
      "最小值位置： tensor([0, 1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "##二维张量的最大值和最小值\n",
    "b=a.reshape(3,4)\n",
    "print(\"2-d张量b:\\n\",b)\n",
    "##最大值及位置（每行）\n",
    "print(\"最大值:\\n\",b.max(dim=1))\n",
    "print(\"最大值位置：\",b.argmax(dim=1))\n",
    "##最小值及位置（每行）\n",
    "print(\"最小值:\\n\",b.min(dim=0))\n",
    "print(\"最小值位置：\",b.argmin(dim=0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.return_types.sort(\nvalues=tensor([99., 67., 55., 44., 34., 32., 30., 29., 25., 23., 12., 11.]),\nindices=tensor([ 8,  4,  9, 11,  1,  5,  7,  6,  2, 10,  0,  3]))"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##张量排序,分别输出从小到大的结果和相应的元素在原始位置的索引\n",
    "print(torch.sort(a))\n",
    "##降序排列\n",
    "print(torch.sort(a, descending=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b sort:\n",
      " tensor([[11., 12., 25., 34.],\n",
      "        [29., 30., 32., 67.],\n",
      "        [23., 44., 55., 99.]])\n",
      "b sort index:\n",
      " tensor([[3, 0, 2, 1],\n",
      "        [2, 3, 1, 0],\n",
      "        [2, 3, 1, 0]])\n",
      "b argsort:\n",
      " tensor([[3, 0, 2, 1],\n",
      "        [2, 3, 1, 0],\n",
      "        [2, 3, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "##对2-d张量进行排序\n",
    "bsort,bsort_id=torch.sort(b)\n",
    "print(\"b sort:\\n\",bsort)\n",
    "print(\"b sort index:\\n\",bsort_id)\n",
    "print(\"b argsort:\\n\",torch.argsort(b))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([99., 67., 55., 44.]),\n",
      "indices=tensor([ 8,  4,  9, 11]))\n",
      "b 每列 top2:\n",
      " tensor([[99., 55., 29., 44.],\n",
      "        [67., 34., 25., 30.]])\n",
      "b 每列 top2 位置:\n",
      " tensor([[2, 2, 1, 2],\n",
      "        [1, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "##获取张量前几个最大的数值\n",
    "print(torch.topk(a, 4))\n",
    "##获取2d张量每列前几个最大的数值\n",
    "btop2,btop2_id=torch.topk(b,2,dim=0)\n",
    "print(\"b 每列 top2:\\n\",btop2)\n",
    "print(\"b 每列 top2 位置:\\n\",btop2_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.return_types.kthvalue(\nvalues=tensor(23.),\nindices=tensor(10))"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##获取张量第k小的数值的位置\n",
    "torch.kthvalue(a,3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[25.],\n        [32.],\n        [55.]])"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##获取2d张量第k小的数值和位置\n",
    "bkth,bkth_id=torch.kthvalue(b,3,dim=1,keepdim=True)\n",
    "bkth"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20.5000],\n",
      "        [39.5000],\n",
      "        [55.2500]])\n",
      "tensor([[59.3333, 40.3333, 25.6667, 28.3333]])\n"
     ]
    }
   ],
   "source": [
    "##平均值，计算每行的平均值\n",
    "print(torch.mean(b,dim=1,keepdim=True))\n",
    "print(torch.mean(b, dim=0, keepdim=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 82.],\n",
      "        [158.],\n",
      "        [221.]])\n",
      "tensor([[178., 121.,  77.,  85.]])\n",
      "tensor([[ 12.,  46.,  71.,  82.],\n",
      "        [ 67.,  99., 128., 158.],\n",
      "        [ 99., 154., 177., 221.]])\n",
      "tensor([[ 12.,  34.,  25.,  11.],\n",
      "        [ 79.,  66.,  54.,  41.],\n",
      "        [178., 121.,  77.,  85.]])\n"
     ]
    }
   ],
   "source": [
    "##计算每行和\n",
    "print(torch.sum(b,dim=1,keepdim=True))\n",
    "##计算每列的和\n",
    "print(torch.sum(b,dim=0,keepdim=True))\n",
    "##按行计算累加和\n",
    "print(torch.cumsum(b, dim=1))\n",
    "##按列计算累加和\n",
    "print(torch.cumsum(b,dim=0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.median(\n",
      "values=tensor([[12.],\n",
      "        [30.],\n",
      "        [44.]]),\n",
      "indices=tensor([[0],\n",
      "        [3],\n",
      "        [3]]))\n",
      "torch.return_types.median(\n",
      "values=tensor([[67., 34., 25., 30.]]),\n",
      "indices=tensor([[1, 0, 0, 1]]))\n"
     ]
    }
   ],
   "source": [
    "##计算每行中位数\n",
    "print(torch.median(b, dim=1, keepdim=True))\n",
    "##计算每列的中位数\n",
    "print(torch.median(b,dim=0,keepdim=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 112200.],\n",
      "        [1865280.],\n",
      "        [5510340.]])\n",
      "tensor([[79596., 59840., 16675., 14520.]])\n"
     ]
    }
   ],
   "source": [
    "##按行计算成积\n",
    "print(torch.prod(b,dim=1,keepdim=True))\n",
    "##按照列计算乘积\n",
    "print(torch.prod(b, dim=0, keepdim=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(25.0108)"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##标准差\n",
    "torch.std(a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.tensor([1.2, 3.4])\n",
    "print(a!=1.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T08:06:06.808917400Z",
     "start_time": "2023-12-24T08:06:03.840216900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
