{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston_X.dtype: float64\n",
      "boston_y.dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\envs\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from sklearn.datasets import load_boston, load_iris\n",
    "\n",
    "##读取波士顿回归数据\n",
    "boston_X, boston_y = load_boston(return_X_y=True)\n",
    "print(\"boston_X.dtype:\", boston_X.dtype)\n",
    "print(\"boston_y.dtype:\", boston_y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_xt.dtype: torch.float32\n",
      "train_yt.dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "##训练集x转化为张量，训练集y转化为张量\n",
    "train_xt = torch.from_numpy(boston_X.astype(np.float32))\n",
    "train_yt = torch.from_numpy(boston_y.astype(np.float32))\n",
    "print(\"train_xt.dtype:\", train_xt.dtype)\n",
    "print(\"train_yt.dtype:\", train_yt.dtype)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_x.shape: torch.Size([64, 13])\n",
      "b_y.shape: torch.Size([64])\n",
      "b_x.dtype: torch.float32\n",
      "b_y.dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "##将训练集转化为张量后，使用TensorDataset将x和y整理到一起\n",
    "train_data = Data.TensorDataset(train_xt, train_yt)\n",
    "##定义一个数据加载器，将训练数据进行批量处理\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_data,  ##使用数据集\n",
    "    batch_size=64,  ##处理样本大小\n",
    "    shuffle=True,  ##每次迭代前打乱数据\n",
    "    num_workers=1,  ##使用两个进程\n",
    ")\n",
    "##检查训练数据集的一个batch的样本的维度是否正确\n",
    "for step, (b_x, b_y) in enumerate(train_loader):\n",
    "    if step > 0:\n",
    "        break\n",
    "## 输出训练图像的尺寸和标签的尺寸及数据类型\n",
    "print(\"b_x.shape:\", b_x.shape)\n",
    "print(\"b_y.shape:\", b_y.shape)\n",
    "print(\"b_x.dtype:\", b_x.dtype)\n",
    "print(\"b_y.dtype:\", b_y.dtype)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_x.dtype: float64\n",
      "irisy.dtype: int32\n"
     ]
    }
   ],
   "source": [
    "##处理分类数据\n",
    "iris_x, irisy = load_iris(return_X_y=True)\n",
    "print(\"iris_x.dtype:\", iris_x.dtype)\n",
    "print(\"irisy.dtype:\", irisy.dtype)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_xt.dtype: torch.int64\n",
      "train_yt.dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "##训练集x转化为张量，训练集y转化为张量\n",
    "train_xt = torch.from_numpy(iris_x.astype(np.int64))\n",
    "train_yt = torch.from_numpy(irisy.astype(np.int64))\n",
    "print(\"train_xt.dtype:\", train_xt.dtype)\n",
    "print(\"train_yt.dtype:\", train_yt.dtype)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_x.shape: torch.Size([10, 4])\n",
      "b_y.shape: torch.Size([10])\n",
      "b_x.dtype: torch.int64\n",
      "b_y.dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "##将训练集转化为张量后，使用data.tensorDataset将x和y整理到一起\n",
    "train_data = Data.TensorDataset(train_xt, train_yt)\n",
    "##定义一个数据加载器，将训练数据进行批量处理\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_data,  ##使用数据集\n",
    "    batch_size=10,  ##处理样本大小,\n",
    "    shuffle=True,  ##每次迭代前打乱数据\n",
    "    num_workers=1,  ##使用两个进程\n",
    ")\n",
    "##检查训练数据集的一个batch的样本的维度是否正确\n",
    "for step, (b_x, b_y) in enumerate(train_loader):\n",
    "    if step > 0:\n",
    "        break\n",
    "## 输出训练图像的尺寸和标签的尺寸及数据类型\n",
    "print(\"b_x.shape:\", b_x.shape)\n",
    "print(\"b_y.shape:\", b_y.shape)\n",
    "print(\"b_x.dtype:\", b_x.dtype)\n",
    "print(\"b_y.dtype:\", b_y.dtype)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchvision.transforms as transfroms\n",
    "from torchvision.datasets import ImageFolder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/26421880 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e673a461e86428dbd9b2c6100dd1e58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/29515 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51bcbdb4ca9d49028ea58c0b27297383"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4422102 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b26ec9789544d2296c392040c45355e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5148 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0be011c9d994f9987630aea2efd9a75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST\\FashionMNIST\\raw\n",
      "\n",
      "train_data的batch数量为： 938\n"
     ]
    }
   ],
   "source": [
    "##使用FashionMNIST数据。准备训练数据集\n",
    "train_data = FashionMNIST(\n",
    "    root=\"./data/FashionMNIST\",  ##数据的路径\n",
    "    train=True,  ##只使用训练数据集\n",
    "    transform=transfroms.ToTensor(),\n",
    "    download=True  ##是否下载数据\n",
    ")\n",
    "##定义一个数据加载器\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_data,  ##使用的数据集\n",
    "    batch_size=64,  ##批处理样本大小\n",
    "    shuffle=True,  ##每次迭代前打乱数据\n",
    "    num_workers=2,  ##使用两个进程\n",
    ")\n",
    "##就算train_loader有多少个batch\n",
    "print(\"train_data的batch数量为：\", len(train_loader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data_x.shape: torch.Size([10000, 1, 28, 28])\n",
      "test_data_y.shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "##对测试集进行处理\n",
    "test_data = FashionMNIST(\n",
    "    root=\"./data/FashionMNIST\",  ##数据的路径\n",
    "    train=False,  ##不使用训练数据集\n",
    "    download=False\n",
    ")\n",
    "##为数据添加一个通道维度，并且取值范围缩放到0~1间\n",
    "test_data_x = test_data.data.type(torch.FloatTensor) / 255.0\n",
    "test_data_x = torch.unsqueeze(test_data_x, dim=1)\n",
    "test_data_y = test_data.targets  ##测试集的标签\n",
    "print(\"test_data_x.shape:\", test_data_x.shape)\n",
    "print(\"test_data_y.shape:\", test_data_y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "##对训练集的预处理\n",
    "train_data_transforms = transfroms.Compose([\n",
    "    transfroms.RandomResizedCrop(224),  ##随机成宽比裁剪为224\n",
    "    transfroms.RandomHorizontalFlip(),  ##依概率p=0.5水平反转\n",
    "    transfroms.ToTensor(),  ##转化为张量并归一化至[0-1]\n",
    "    ##图像标准化处理\n",
    "    transfroms.Normalize([\n",
    "        0.485, 0.456, 0.406\n",
    "    ], [\n",
    "        0.229,0.224,0.225\n",
    "    ])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in D:/Rookie/pytorch/data/.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_10768\\853395410.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m##读取图像\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mtrain_data_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"D:/Rookie/pytorch/data/\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mtest_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mImageFolder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data_dir\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrain_data_transforms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mtrain_data_loader\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mData\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataLoader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mshuffle\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mnum_workers\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"数据的lable：\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtargets\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\datasets\\folder.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001B[0m\n\u001B[0;32m    307\u001B[0m         \u001B[0mis_valid_file\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mCallable\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    308\u001B[0m     ):\n\u001B[1;32m--> 309\u001B[1;33m         super().__init__(\n\u001B[0m\u001B[0;32m    310\u001B[0m             \u001B[0mroot\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    311\u001B[0m             \u001B[0mloader\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\datasets\\folder.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001B[0m\n\u001B[0;32m    142\u001B[0m     ) -> None:\n\u001B[0;32m    143\u001B[0m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget_transform\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtarget_transform\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 144\u001B[1;33m         \u001B[0mclasses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclass_to_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind_classes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    145\u001B[0m         \u001B[0msamples\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmake_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclass_to_idx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mextensions\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mis_valid_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    146\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\datasets\\folder.py\u001B[0m in \u001B[0;36mfind_classes\u001B[1;34m(self, directory)\u001B[0m\n\u001B[0;32m    216\u001B[0m             \u001B[1;33m(\u001B[0m\u001B[0mTuple\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mList\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mDict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mList\u001B[0m \u001B[0mof\u001B[0m \u001B[0mall\u001B[0m \u001B[0mclasses\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mdictionary\u001B[0m \u001B[0mmapping\u001B[0m \u001B[0meach\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mto\u001B[0m \u001B[0man\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    217\u001B[0m         \"\"\"\n\u001B[1;32m--> 218\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mfind_classes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdirectory\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    219\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTuple\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mAny\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mAny\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\datasets\\folder.py\u001B[0m in \u001B[0;36mfind_classes\u001B[1;34m(directory)\u001B[0m\n\u001B[0;32m     40\u001B[0m     \u001B[0mclasses\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msorted\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mentry\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mentry\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscandir\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdirectory\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mentry\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_dir\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mclasses\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mFileNotFoundError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Couldn't find any class folder in {directory}.\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m     \u001B[0mclass_to_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[0mcls_name\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls_name\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclasses\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: Couldn't find any class folder in D:/Rookie/pytorch/data/."
     ]
    }
   ],
   "source": [
    "##读取图像\n",
    "train_data_dir=\"D:/Rookie/pytorch/data/\"\n",
    "test_data=ImageFolder(train_data_dir,transform=train_data_transforms)\n",
    "train_data_loader=Data.DataLoader(train_data,batch_size=4,shuffle=True,num_workers=1)\n",
    "print(\"数据的lable：\",train_data.targets)\n",
    "##获得一个batch数据\n",
    "for step,(b_x,b_y) in enumerate(train_data_loader):\n",
    "    if step>0:\n",
    "        break\n",
    "##输出训练图像的尺寸的标签和chicun\n",
    "print(b_x.shape)\n",
    "print(b_y.shape)\n",
    "print(\"图像取值范围为：\",b_x.min(),\"~\",b_x.max())"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
