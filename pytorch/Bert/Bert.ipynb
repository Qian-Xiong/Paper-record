{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-03-13T08:56:58.994321Z",
     "start_time": "2024-03-13T08:56:58.987698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'3.11.4 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 13:38:37) [MSC v.1916 64 bit (AMD64)]'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7bdbe205963f27",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T08:57:02.089442Z",
     "start_time": "2024-03-13T08:56:58.994321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('2.1.0', '0.17.0.dev20230909+cu121', '2.2.0.dev20230909+cu121')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "\n",
    "torch.__version__, torchvision.__version__, torchaudio.__version__"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'4.38.2'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T08:57:02.093863Z",
     "start_time": "2024-03-13T08:57:02.090446Z"
    }
   },
   "id": "69f44e91a0edcfcd",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'2.14.5'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "datasets.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T08:57:02.831622Z",
     "start_time": "2024-03-13T08:57:02.094799Z"
    }
   },
   "id": "96a7d7639c7d1c46",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'0.6.0'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "torchtext.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T08:57:02.853025Z",
     "start_time": "2024-03-13T08:57:02.832626Z"
    }
   },
   "id": "f5b6659120cd8d9d",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca419d5a5777495992bb6319781f93f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fef4b9bd3664414d8f410defce6d0d90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "233c5e1c2aeb44b7a12962f20ce936e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62def9443659496286e220562761a8e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n \t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n },\n ['选择珠江花园的原因就是方便。',\n  '笔记本的键盘确实爽。',\n  '房间太小。其他的都一般。',\n  '今天才知道这书还有第6卷,真有点郁闷.',\n  '机器背面似乎被撕了张什么标签，残胶还在。'])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path='bert-base-chinese',\n",
    "                                          cache_dir=None,\n",
    "                                          force_download=False)\n",
    "sents = [\n",
    "    '选择珠江花园的原因就是方便。',\n",
    "    '笔记本的键盘确实爽。',\n",
    "    '房间太小。其他的都一般。',\n",
    "    '今天才知道这书还有第6卷,真有点郁闷.',\n",
    "    '机器背面似乎被撕了张什么标签，残胶还在。'\n",
    "]\n",
    "tokenizer, sents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T08:57:07.614372Z",
     "start_time": "2024-03-13T08:57:02.853025Z"
    }
   },
   "id": "705ad4eb6d8e6222",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "'[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tokenizer.encode(\n",
    "    text=sents[0],\n",
    "    text_pair=sents[1],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    add_special_tokens=True,\n",
    "    max_length=30,\n",
    "    return_tensors=None\n",
    ")\n",
    "print(out)\n",
    "tokenizer.decode(out)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T08:57:07.636915Z",
     "start_time": "2024-03-13T08:57:07.615376Z"
    }
   },
   "id": "2601a17e478662e9",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'return_tensors_info': True} not recognized.\n",
      "Keyword arguments {'return_tensors_info': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]\n",
      "token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "length : 30\n"
     ]
    },
    {
     "data": {
      "text/plain": "'[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tokenizer.encode_plus(\n",
    "    text=sents[0],\n",
    "    text_pair=sents[1],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=30,\n",
    "    add_special_tokens=True,\n",
    "\n",
    "    return_tensors=None,\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors_info=True,\n",
    "    return_length=True\n",
    ")\n",
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "tokenizer.decode(out[\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T09:02:32.796269Z",
     "start_time": "2024-03-13T09:02:32.788921Z"
    }
   },
   "id": "6667fdd033d04b28",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'return_tensors_info': True} not recognized.\n",
      "Keyword arguments {'return_tensors_info': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 102], [101, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]]\n",
      "length : [15, 12]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "('[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 [SEP]',\n '[CLS] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##批量编码句子\n",
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[sents[0], sents[1]],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=15,\n",
    "    add_special_tokens=True,\n",
    "\n",
    "    return_tensors=None,\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors_info=True,\n",
    "    return_special_tokens_mask=True,\n",
    "    return_length=True\n",
    ")\n",
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "tokenizer.decode(out[\"input_ids\"][0]), tokenizer.decode(out['input_ids'][1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T09:08:46.256246Z",
     "start_time": "2024-03-13T09:08:46.245593Z"
    }
   },
   "id": "10b5d65b3e5e6e0f",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'return_tensors_info': True} not recognized.\n",
      "Keyword arguments {'return_tensors_info': True} not recognized.\n",
      "Keyword arguments {'return_tensors_info': True} not recognized.\n",
      "Keyword arguments {'return_tensors_info': True} not recognized.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0], [101, 2791, 7313, 1922, 2207, 511, 1071, 800, 4638, 6963, 671, 5663, 511, 102, 791, 1921, 2798, 4761, 6887, 6821, 741, 6820, 3300, 5018, 127, 1318, 117, 4696, 3300, 102]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
      "length : [27, 30]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "('[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]',\n '[CLS] 房 间 太 小 。 其 他 的 都 一 般 。 [SEP] 今 天 才 知 道 这 书 还 有 第 6 卷, 真 有 [SEP]')"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##批量编码句子对\n",
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[(sents[0], sents[1]), (sents[2], sents[3])],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=30,\n",
    "    add_special_tokens=True,\n",
    "\n",
    "    return_tensors=None,\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors_info=True,\n",
    "    return_special_tokens_mask=True,\n",
    "    return_length=True\n",
    ")\n",
    "for k, v in out.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "tokenizer.decode(out[\"input_ids\"][0]), tokenizer.decode(out['input_ids'][1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T09:10:30.889721Z",
     "start_time": "2024-03-13T09:10:30.881763Z"
    }
   },
   "id": "a24712bc984d7d39",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(dict, 21128, False)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取字典\n",
    "dict = tokenizer.get_vocab()\n",
    "type(dict), len(dict), '月光' in dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T09:12:59.272458Z",
     "start_time": "2024-03-13T09:12:59.266460Z"
    }
   },
   "id": "358b2d3553d5ad43",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(dict, 21131, 21128, 21130)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(new_tokens=['月光', '希望'])\n",
    "tokenizer.add_special_tokens({'eos_token': '[EOS]'})\n",
    "dict = tokenizer.get_vocab()\n",
    "type(dict), len(dict), dict['月光'], dict['[EOS]']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T09:19:18.839772Z",
     "start_time": "2024-03-13T09:19:18.831205Z"
    }
   },
   "id": "88adbc022c2b49b5",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'text_pairs': None} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 21128, 4638, 3173, 21129, 21130, 102, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "'[CLS] 月光 的 新 希望 [EOS] [SEP] [PAD]'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tokenizer.encode(\n",
    "    text='月光的新希望[EOS]',\n",
    "    text_pairs=None,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    add_special_tokens=True,\n",
    "    max_length=8,\n",
    "    return_tensors=None\n",
    ")\n",
    "print(out)\n",
    "tokenizer.decode(out)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T09:19:31.343458Z",
     "start_time": "2024-03-13T09:19:31.338169Z"
    }
   },
   "id": "417d57eabcc0457a",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[37], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m##下载数据\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset\n\u001B[1;32m----> 3\u001B[0m dataset \u001B[38;5;241m=\u001B[39m load_dataset(path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlansinuote/ChnSentiCorp\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m dataset\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\datasets\\load.py:2166\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001B[0m\n\u001B[0;32m   2162\u001B[0m \u001B[38;5;66;03m# Build dataset for splits\u001B[39;00m\n\u001B[0;32m   2163\u001B[0m keep_in_memory \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2164\u001B[0m     keep_in_memory \u001B[38;5;28;01mif\u001B[39;00m keep_in_memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m is_small_dataset(builder_instance\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size)\n\u001B[0;32m   2165\u001B[0m )\n\u001B[1;32m-> 2166\u001B[0m ds \u001B[38;5;241m=\u001B[39m builder_instance\u001B[38;5;241m.\u001B[39mas_dataset(split\u001B[38;5;241m=\u001B[39msplit, verification_mode\u001B[38;5;241m=\u001B[39mverification_mode, in_memory\u001B[38;5;241m=\u001B[39mkeep_in_memory)\n\u001B[0;32m   2167\u001B[0m \u001B[38;5;66;03m# Rename and cast features to match task schema\u001B[39;00m\n\u001B[0;32m   2168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m task \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2169\u001B[0m     \u001B[38;5;66;03m# To avoid issuing the same warning twice\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:1173\u001B[0m, in \u001B[0;36mDatasetBuilder.as_dataset\u001B[1;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001B[0m\n\u001B[0;32m   1171\u001B[0m is_local \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m is_remote_filesystem(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fs)\n\u001B[0;32m   1172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_local:\n\u001B[1;32m-> 1173\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading a dataset cached in a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fs)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not supported.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_dir):\n\u001B[0;32m   1175\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[0;32m   1176\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: could not find data in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Please make sure to call \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1177\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilder.download_and_prepare(), or use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1178\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatasets.load_dataset() before trying to access the Dataset object.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1179\u001B[0m     )\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: Loading a dataset cached in a LocalFileSystem is not supported."
     ]
    }
   ],
   "source": [
    "##下载数据\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(path='lansinuote/ChnSentiCorp')\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T11:34:59.084578Z",
     "start_time": "2024-03-13T11:34:45.690999Z"
    }
   },
   "id": "c5942f1979f93f1c",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 9600\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1200\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1200\n    })\n})"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#从本地下载数据\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('file://D:\\Rookie\\pytorch\\Bert\\ChnSentiCorp')\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T11:36:43.957488Z",
     "start_time": "2024-03-13T11:36:43.883283Z"
    }
   },
   "id": "fa0a0d81274cb966",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label'],\n    num_rows: 9600\n})"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset['train']\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T11:45:01.186524Z",
     "start_time": "2024-03-13T11:45:01.182038Z"
    }
   },
   "id": "548bdb207e816ae8",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n 'label': 1}"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T11:45:38.057251Z",
     "start_time": "2024-03-13T11:45:38.047859Z"
    }
   },
   "id": "8a63054dc94673d8",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 8640\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 960\n    })\n})"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_test_split(test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T11:50:26.533812Z",
     "start_time": "2024-03-13T11:50:26.500904Z"
    }
   },
   "id": "e534f0aea078947",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label'],\n    num_rows: 2400\n})"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shard(num_shards=4,index=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T11:51:00.190772Z",
     "start_time": "2024-03-13T11:51:00.182764Z"
    }
   },
   "id": "8535e9e8261166bf",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[62], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m##导出为csv格式\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m dataset\u001B[38;5;241m.\u001B[39mto_csv(path_or_buf\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/ChnSentiCorp.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m dataset\u001B[38;5;241m=\u001B[39mload_from_disk(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfile://D:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mRookie\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mpytorch\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mBert\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mChnSentiCorp\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m##加载csv格式数据\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'DatasetDict' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "##导出为csv格式\n",
    "dataset.to_csv(path_or_buf='./data/ChnSentiCorp.csv')\n",
    "dataset=load_from_disk('file://D:\\Rookie\\pytorch\\Bert\\ChnSentiCorp')\n",
    "##加载csv格式数据\n",
    "csv_dataset=load_dataset(path='csv',\n",
    "                         data_files='file://D:\\Rookie\\pytorch\\Bert\\ChnSentiCorp.csv',\n",
    "                         split='train')\n",
    "csv_dataset[20]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T12:04:01.375964Z",
     "start_time": "2024-03-13T12:04:01.364668Z"
    }
   },
   "id": "675ec8964ac0bf2",
   "execution_count": 62
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
